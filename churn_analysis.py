#!/usr/bin/env python
# coding: utf-8

# # Customer Churn Prediction Projesi
# 
# ## GiriÅŸ ve AmaÃ§
# Bu projede, bir telekomÃ¼nikasyon ÅŸirketinin mÃ¼ÅŸteri kaybÄ±nÄ± (churn) tahmin etmek amaÃ§lanmÄ±ÅŸtÄ±r.  
# MÃ¼ÅŸteri kaybÄ±nÄ± Ã¶nlemek iÃ§in erken uyarÄ± sistemleri geliÅŸtirilerek, risk altÄ±ndaki mÃ¼ÅŸterilere yÃ¶nelik stratejiler Ã¶nerilecektir.  
# 
# ## Veri Seti
# - Kaynak: Kaggle - Telco Customer Churn  
# - MÃ¼ÅŸteri sayÄ±sÄ±: 7043  
# - Ã–zellik sayÄ±sÄ±: 21 (demografik bilgiler, hizmet kullanÄ±mÄ±, Ã¶deme bilgileri vb.)  
# 
# Bu veri seti Ã¼zerinde veri Ã¶n iÅŸleme, keÅŸifsel veri analizi ve farklÄ± makine Ã¶ÄŸrenmesi modelleri ile churn tahmini yapÄ±lmÄ±ÅŸtÄ±r.

# ## Veri Ä°ncelemesi ve Ã–n Ä°ÅŸleme
# 
# Veri setinin temel yapÄ±sÄ±, eksik veri durumu ve veri tÃ¼rleri incelenmiÅŸtir.  
# Eksik veri yoktur ancak `TotalCharges` sÃ¼tunu obje tipinde olup sayÄ±sal verilere Ã§evrilmiÅŸtir.  
# 
# Kategorik deÄŸiÅŸkenler one-hot encoding ile sayÄ±sal hale getirilmiÅŸtir.  
# SayÄ±sal deÄŸiÅŸkenler Ã¶lÃ§eklendirilmiÅŸ, veri dengelenmesi class_weight kullanÄ±lmÄ±ÅŸtÄ±r.

# In[97]:


import pandas as pd

# CSV dosyasÄ±nÄ± oku
df = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")

# Ä°lk 5 satÄ±rÄ± gÃ¶rÃ¼ntÃ¼le
df.head()


# In[7]:


# Veri Ã§erÃ§evesinin boyutu (kaÃ§ satÄ±r, kaÃ§ sÃ¼tun)
print("SatÄ±r sayÄ±sÄ±:", df.shape[0])
print("SÃ¼tun sayÄ±sÄ±:", df.shape[1])

# SÃ¼tun isimleri ve veri tipleri
df.dtypes


# In[8]:


# Genel veri tipi ve eksik veri kontrolÃ¼
df.info()


# In[9]:


# SayÄ±sal sÃ¼tunlarÄ±n Ã¶zet istatistikleri
df.describe()


# In[10]:


# TotalCharges'Ä± sayÄ±sal deÄŸere Ã§evir (hatalÄ± olanlarÄ± NaN yapar)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')


# In[11]:


# Her sÃ¼tunda kaÃ§ eksik deÄŸer var?
df.isnull().sum()


# In[12]:


df.dropna(inplace=True)


# ## KeÅŸifsel Veri Analizi (EDA)
# 
# Veri setindeki churn daÄŸÄ±lÄ±mÄ±, demografik Ã¶zellikler ve abonelik bilgileri incelenmiÅŸtir.  
# Ã–nemli gÃ¶zlemler:
# 
# - MÃ¼ÅŸteri kaybÄ± yaÅŸayanlarÄ±n oranÄ± yaklaÅŸÄ±k %26.
# - SÃ¶zleÅŸme tÃ¼rÃ¼ (Contract) churn Ã¼zerinde belirgin etkiye sahip.
# - Abonelik sÃ¼resi (tenure) arttÄ±kÃ§a churn oranÄ± azalmakta.

# In[13]:


import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='Churn', data=df)
plt.title("Churn DaÄŸÄ±lÄ±mÄ±")
plt.show()


# ### Churn DaÄŸÄ±lÄ±mÄ±
# 
# Bu grafik, veri setimizdeki mÃ¼ÅŸterilerin "churn" durumuna gÃ¶re daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶stermektedir.  
# - "No" etiketi, mÃ¼ÅŸteri kaybÄ± yaÅŸamayanlarÄ±,  
# - "Yes" etiketi ise mÃ¼ÅŸteri kaybÄ± yaÅŸayanlarÄ± temsil etmektedir.  
# 
# Grafikten gÃ¶rÃ¼ldÃ¼ÄŸÃ¼ gibi, mÃ¼ÅŸteri kaybÄ± yaÅŸamayanlarÄ±n sayÄ±sÄ±, kaybedenlere gÃ¶re daha fazladÄ±r.  
# Bu dengesizlik modellemeye etki edeceÄŸi iÃ§in, class_weight yÃ¶ntemi ile denge saÄŸlanmÄ±ÅŸtÄ±r.

# In[14]:


df.describe()


# In[23]:


sns.countplot(x='Contract', hue='Churn', data=df)
plt.title("Churn - Contract TÃ¼rÃ¼ne GÃ¶re DaÄŸÄ±lÄ±mÄ±")
plt.show()


# ### Churn - Contract TÃ¼rÃ¼ne GÃ¶re DaÄŸÄ±lÄ±mÄ±
# 
# Bu grafik, mÃ¼ÅŸterilerin sÃ¶zleÅŸme tÃ¼rlerine gÃ¶re churn durumlarÄ±nÄ± gÃ¶stermektedir. 
# 
# - **Month-to-month (AylÄ±k)** sÃ¶zleÅŸmeye sahip mÃ¼ÅŸterilerde churn oranÄ± oldukÃ§a yÃ¼ksektir, yani bu mÃ¼ÅŸteriler daha sÄ±k aboneliklerini iptal etmektedir.
# - **One year (1 yÄ±llÄ±k)** sÃ¶zleÅŸmeye sahip mÃ¼ÅŸterilerde churn oranÄ± daha dÃ¼ÅŸÃ¼k seviyededir.
# - **Two year (2 yÄ±llÄ±k)** sÃ¶zleÅŸme kullanan mÃ¼ÅŸteriler ise en dÃ¼ÅŸÃ¼k churn oranÄ±na sahiptir ve sadakatleri daha yÃ¼ksektir.
# 
# Bu sonuÃ§lar, uzun dÃ¶nemli sÃ¶zleÅŸmelerin mÃ¼ÅŸteri kaybÄ±nÄ± azaltmada etkili olduÄŸunu gÃ¶stermektedir. Ä°ÅŸletmeler, churn riskini azaltmak iÃ§in uzun sÃ¼reli sÃ¶zleÅŸmeleri teÅŸvik edebilir.

# In[16]:


sns.boxplot(x='Churn', y='tenure', data=df)
plt.title("Churn vs Tenure")
plt.show()


# ### Churn ve Tenure Ä°liÅŸkisi
# 
# Bu kutu grafiÄŸi (box plot), mÃ¼ÅŸterilerin abonelik sÃ¼resi (tenure) ile churn (abonelik iptali) durumlarÄ± arasÄ±ndaki iliÅŸkiyi gÃ¶stermektedir.
# 
# - **Churn etmeyen mÃ¼ÅŸteriler (No)** genellikle daha uzun sÃ¼redir aboneliklerini devam ettirmektedir. Bu mÃ¼ÅŸterilerin tenure medyanÄ± ve daÄŸÄ±lÄ±mÄ± daha yÃ¼ksektir.
# - **Churn eden mÃ¼ÅŸteriler (Yes)** ise aboneliklerini daha kÄ±sa bir sÃ¼re sonra iptal etmektedir. Tenure medyanÄ± daha dÃ¼ÅŸÃ¼k ve daÄŸÄ±lÄ±m daha sÄ±nÄ±rlÄ±dÄ±r.
# 
# Bu sonuÃ§, abonelik sÃ¼resi arttÄ±kÃ§a mÃ¼ÅŸteri baÄŸlÄ±lÄ±ÄŸÄ±nÄ±n ve kalÄ±cÄ±lÄ±ÄŸÄ±nÄ±n da arttÄ±ÄŸÄ±nÄ± gÃ¶stermektedir. MÃ¼ÅŸteri kaybÄ±nÄ± azaltmak iÃ§in yeni mÃ¼ÅŸterilerin abonelik sÃ¼relerini artÄ±rmaya yÃ¶nelik stratejiler geliÅŸtirilebilir.

# In[17]:


corr = df.corr(numeric_only=True)
sns.heatmap(corr, annot=True, cmap="coolwarm")
plt.title("Korelasyon Matrisi")
plt.show()


# ### Korelasyon Matrisi
# 
# Bu korelasyon matrisi, seÃ§ilen sayÄ±sal deÄŸiÅŸkenler arasÄ±ndaki iliÅŸkileri gÃ¶stermektedir:
# 
# - **tenure ile TotalCharges arasÄ±nda yÃ¼ksek pozitif korelasyon (0.83)** bulunmaktadÄ±r. Bu, abonelik sÃ¼resi uzadÄ±kÃ§a toplam Ã¼cretlerin de arttÄ±ÄŸÄ±nÄ± gÃ¶sterir.
# - **MonthlyCharges ile TotalCharges arasÄ±nda da gÃ¼Ã§lÃ¼ bir pozitif iliÅŸki (0.65)** vardÄ±r.
# - **SeniorCitizen deÄŸiÅŸkeni diÄŸer deÄŸiÅŸkenlerle zayÄ±f korelasyona** sahiptir.
# - DiÄŸer deÄŸiÅŸkenler arasÄ±nda dÃ¼ÅŸÃ¼k veya orta derecede pozitif iliÅŸkiler bulunmaktadÄ±r.
# 
# Bu analiz, modelde kullanÄ±lan deÄŸiÅŸkenlerin birbirleriyle olan iliÅŸkilerini anlamamÄ±za ve olasÄ± Ã§oklu baÄŸlantÄ±larÄ± tespit etmemize yardÄ±mcÄ± olur.

# In[18]:


sns.countplot(x='InternetService', hue='Churn', data=df)
plt.title("Churn - Internet Service'e GÃ¶re DaÄŸÄ±lÄ±m")
plt.show()


# **ğŸŒ Ä°nternet Servis TÃ¼rÃ¼ne GÃ¶re Churn OranÄ±:**
# 
# Fiber optik kullanÄ±cÄ±larÄ±nÄ±n churn oranÄ± diÄŸer internet tÃ¼rlerine gÃ¶re oldukÃ§a yÃ¼ksektir. Bu durum, fiber kullanÄ±cÄ±larÄ±nÄ±n beklentilerinin karÅŸÄ±lanmamasÄ± veya fiyat/hizmet dengesinde memnuniyetsizlik yaÅŸadÄ±ÄŸÄ±nÄ± gÃ¶sterebilir. Ã–te yandan, internet hizmeti almayan kullanÄ±cÄ±lar neredeyse hiÃ§ churn gÃ¶stermemektedir; bu da hizmet baÄŸlÄ±lÄ±ÄŸÄ±nÄ±n churn Ã¼zerinde etkili olduÄŸunu ortaya koyar.

# In[19]:


sns.countplot(x='OnlineSecurity', hue='Churn', data=df)
plt.title("Churn - Online Security'e GÃ¶re DaÄŸÄ±lÄ±m")
plt.show()


# **ğŸ” Online GÃ¼venlik Hizmetine GÃ¶re Churn OranÄ±:**
# 
# Online gÃ¼venlik hizmeti almayan kullanÄ±cÄ±larÄ±n churn oranÄ±, hizmet alanlara kÄ±yasla belirgin ÅŸekilde daha yÃ¼ksektir. Bu durum, gÃ¼venlik hizmetinin mÃ¼ÅŸteri baÄŸlÄ±lÄ±ÄŸÄ±nÄ± artÄ±rmada Ã¶nemli bir rol oynadÄ±ÄŸÄ±nÄ± gÃ¶stermektedir. GÃ¼venlik hissiyatÄ±, kullanÄ±cÄ± deneyimi Ã¼zerinde doÄŸrudan etkili olabilir ve hizmetten memnuniyeti artÄ±rabilir.

# In[20]:


sns.countplot(x='Contract', hue='Churn', data=df)
plt.title("Churn - Contract TÃ¼rÃ¼ne GÃ¶re DaÄŸÄ±lÄ±m")
plt.show()


# **ğŸ“„ SÃ¶zleÅŸme TÃ¼rÃ¼ne GÃ¶re Churn OranÄ±:**
# 
# Churn oranÄ±, sÃ¶zleÅŸme sÃ¼resi kÄ±saldÄ±kÃ§a artmaktadÄ±r. AylÄ±k (month-to-month) sÃ¶zleÅŸme yapan mÃ¼ÅŸterilerde churn oranÄ± oldukÃ§a yÃ¼ksekken, 1 yÄ±llÄ±k ve Ã¶zellikle 2 yÄ±llÄ±k sÃ¶zleÅŸmeye sahip mÃ¼ÅŸterilerde bu oran Ã¶nemli Ã¶lÃ§Ã¼de dÃ¼ÅŸmektedir. Bu da uzun vadeli sÃ¶zleÅŸmelerin mÃ¼ÅŸteri sadakatini artÄ±rdÄ±ÄŸÄ±nÄ± ve churn riskini azalttÄ±ÄŸÄ±nÄ± gÃ¶stermektedir.

# In[21]:


sns.countplot(x='TechSupport', hue='Churn', data=df)
plt.title("Churn - Tech Support'a GÃ¶re DaÄŸÄ±lÄ±m")
plt.show()


# **ğŸ’¡ Teknik Destek Hizmetine GÃ¶re Churn OranÄ±:**
# 
# Teknik destek hizmeti almayan mÃ¼ÅŸterilerin churn oranÄ±, destek hizmeti alanlara gÃ¶re belirgin ÅŸekilde daha yÃ¼ksektir. Bu durum, teknik desteÄŸin mÃ¼ÅŸteri memnuniyeti ve baÄŸlÄ±lÄ±ÄŸÄ±nda Ã¶nemli bir rol oynadÄ±ÄŸÄ±nÄ± gÃ¶stermektedir. Destek hizmeti sunmak, churn'Ã¼ azaltmada etkili bir strateji olabilir.

# In[22]:


sns.countplot(x='PaymentMethod', hue='Churn', data=df)
plt.title("Churn - Payment Method'a GÃ¶re DaÄŸÄ±lÄ±m")
plt.xticks(rotation=45)
plt.show()


# **ğŸ’³ Ã–deme YÃ¶ntemine GÃ¶re Churn OranÄ±:**
# 
# "Electronic check" Ã¶deme yÃ¶ntemini kullanan mÃ¼ÅŸteriler arasÄ±nda churn oranÄ± diÄŸer yÃ¶ntemlere kÄ±yasla oldukÃ§a yÃ¼ksektir. Buna karÅŸÄ±n otomatik Ã¶deme yÃ¶ntemleri (Ã¶rneÄŸin banka transferi ve kredi kartÄ±) kullanan mÃ¼ÅŸterilerde churn oranÄ± anlamlÄ± derecede dÃ¼ÅŸÃ¼ktÃ¼r. Bu durum, otomatik Ã¶deme sistemlerinin mÃ¼ÅŸteri sadakatini artÄ±rabileceÄŸini gÃ¶stermektedir.

# ## Modeller
# 
# Proje kapsamÄ±nda aÅŸaÄŸÄ±daki modeller kullanÄ±lmÄ±ÅŸtÄ±r:
# 
# - Logistic Regression  
# - Random Forest  
# - XGBoost  
# - LightGBM  
# - CatBoost (Final model)
# 
# Her model hiperparametre ayarlarÄ± yapÄ±larak performanslarÄ± karÅŸÄ±laÅŸtÄ±rÄ±lmÄ±ÅŸtÄ±r.

# In[25]:


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# 1. Kategorik verileri sayÄ±sal formata Ã§evir (one-hot encoding)
df_encoded = pd.get_dummies(df, drop_first=True)

# 2. X ve y tanÄ±mÄ± (baÄŸÄ±msÄ±z ve baÄŸÄ±mlÄ± deÄŸiÅŸken)
X = df_encoded.drop('Churn_Yes', axis=1)
y = df_encoded['Churn_Yes']

# 3. Verileri eÄŸitim ve test olarak bÃ¶l
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Modeli oluÅŸtur ve eÄŸit
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# 5. Tahmin yap ve sonuÃ§larÄ± deÄŸerlendir
y_pred = model.predict(X_test)

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))


# In[26]:


from sklearn.preprocessing import StandardScaler

# 1. SayÄ±sal kolonlarÄ± belirle
numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']

# 2. Scaler nesnesini oluÅŸtur
scaler = StandardScaler()

# 3. Ã–lÃ§ekleme iÅŸlemi (X'e uygulayacaÄŸÄ±z)
df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])


# In[27]:


df_encoded[numeric_cols].head()


# In[28]:


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# 1. X ve y tanÄ±mÄ± (baÄŸÄ±msÄ±z ve baÄŸÄ±mlÄ± deÄŸiÅŸkenler)
X = df_encoded.drop('Churn_Yes', axis=1)
y = df_encoded['Churn_Yes']

# 2. EÄŸitim ve test setlerine ayÄ±r
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Modeli oluÅŸtur
model = LogisticRegression(max_iter=2000)

# 4. EÄŸit
model.fit(X_train, y_train)

# 5. Tahmin yap
y_pred = model.predict(X_test)

# 6. SonuÃ§larÄ± deÄŸerlendir
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))


# In[31]:


# customerID gibi anlamsÄ±z sÃ¼tunlarÄ± Ã§Ä±kar
df_model = df.drop(columns=['customerID'])

# Kategorik ve sayÄ±sal deÄŸiÅŸkenleri ayÄ±r
cat_cols = df_model.select_dtypes(include='object').columns
cat_cols = cat_cols.drop('Churn')  # â¬…ï¸ Churn'u Ã§Ä±karÄ±yoruz
num_cols = df_model.select_dtypes(exclude='object').columns

# Churn deÄŸiÅŸkenini hedef deÄŸiÅŸken olarak ayÄ±r
X = df_model.drop('Churn', axis=1)
y = df_model['Churn'].map({'No': 0, 'Yes': 1})

# Sadece kategorik sÃ¼tunlara one-hot encoding uygula
X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=True)

# EÄŸitim ve test setlerine ayÄ±r
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)


# In[32]:


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Modeli oluÅŸtur ve eÄŸit
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Tahmin yap
y_pred = model.predict(X_test)

# SonuÃ§larÄ± deÄŸerlendir
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))


# In[47]:


from sklearn.linear_model import LogisticRegression
model = LogisticRegression(class_weight='balanced')
# Modeli eÄŸit
model.fit(X_train, y_train)

# Tahmin yap
y_pred = model.predict(X_test)

# Performans metriklerini hesapla
from sklearn.metrics import classification_report, confusion_matrix

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


# In[48]:


from sklearn.linear_model import LogisticRegression
model = LogisticRegression(class_weight='balanced', max_iter=1000)

# Modeli eÄŸit
model.fit(X_train, y_train)

# Tahmin yap
y_pred = model.predict(X_test)

# Performans metriklerini hesapla
from sklearn.metrics import classification_report, confusion_matrix

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


# In[51]:


from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline

# Pipeline ile Ã¶lÃ§ekleme ve modeli baÄŸla
model = make_pipeline(
    StandardScaler(),
    LogisticRegression(class_weight='balanced', max_iter=2000)
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix

# Confusion Matrix ve Classification Report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


# In[52]:


from sklearn.metrics import roc_auc_score

y_pred_proba = model.predict_proba(X_test)[:, 1]
roc_auc = roc_auc_score(y_test, y_pred_proba)
print("ROC AUC:", roc_auc)


# In[53]:


from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')
print("Average F1 (CV):", scores.mean())


# In[54]:


from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.model_selection import cross_val_score

# Pipeline ile model
model = make_pipeline(
    StandardScaler(),
    RandomForestClassifier(class_weight='balanced', random_state=42)
)

# EÄŸit
model.fit(X_train, y_train)

# Tahmin
y_pred = model.predict(X_test)

# Performans
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# ROC AUC
y_pred_proba = model.predict_proba(X_test)[:, 1]
roc_auc = roc_auc_score(y_test, y_pred_proba)
print("ROC AUC:", roc_auc)

# Cross-validation (5-fold F1)
scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')
print("Average F1 (CV):", scores.mean())


# In[55]:


from sklearn.model_selection import GridSearchCV

param_grid = {
    'randomforestclassifier__n_estimators': [100, 200],
    'randomforestclassifier__max_depth': [None, 10, 20],
    'randomforestclassifier__min_samples_split': [2, 5],
}

grid = GridSearchCV(model, param_grid, cv=5, scoring='f1', n_jobs=-1)
grid.fit(X_train, y_train)

print("Best Params:", grid.best_params_)
print("Best CV F1 Score:", grid.best_score_)


# In[56]:


from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# En iyi modeli al
best_model = grid.best_estimator_

# Test verisiyle tahmin
y_pred = best_model.predict(X_test)
y_pred_proba = best_model.predict_proba(X_test)[:, 1]

# Metrikler
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("ROC AUC:", roc_auc_score(y_test, y_pred_proba))


# In[57]:


import pandas as pd
import matplotlib.pyplot as plt

# Ã–zellik isimlerini al (X_train bir DataFrame ise)
feature_names = X_train.columns

# Ã–nem deÄŸerlerini al
importances = best_model.named_steps['randomforestclassifier'].feature_importances_

# DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r ve sÄ±rala
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Ä°lk 10 Ã¶zelliÄŸi Ã§izdir
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'][:10][::-1], feature_importance_df['Importance'][:10][::-1])
plt.xlabel('Importance')
plt.title('Top 10 Feature Importances (Random Forest)')
plt.tight_layout()
plt.show()


# In[59]:


from xgboost import XGBClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# Pipeline: StandardScaler + XGBoost
model = make_pipeline(
    StandardScaler(),
    XGBClassifier(
        objective='binary:logistic',
        eval_metric='logloss',
        use_label_encoder=False,
        scale_pos_weight=(y_train.value_counts()[0] / y_train.value_counts()[1]),  # class imbalance iÃ§in
        random_state=42
    )
)

# EÄŸit
model.fit(X_train, y_train)

# Tahmin
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# DeÄŸerlendirme
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("ROC AUC:", roc_auc_score(y_test, y_pred_proba))


# In[61]:


from lightgbm import LGBMClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score

# Model pipeline: scaler + LGBM
model = make_pipeline(
    StandardScaler(),
    LGBMClassifier(
        class_weight='balanced',
        random_state=42,
        n_estimators=100,
        boosting_type='gbdt'
    )
)

# EÄŸit
model.fit(X_train, y_train)

# Tahmin
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# DeÄŸerlendirme
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("ROC AUC:", roc_auc_score(y_test, y_pred_proba))


# In[63]:


from catboost import CatBoostClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score

# SÄ±nÄ±f aÄŸÄ±rlÄ±klarÄ± (0 ve 1 sÄ±nÄ±fÄ± iÃ§in)
class_weights = {
    0: y_train.value_counts()[1] / len(y_train),
    1: y_train.value_counts()[0] / len(y_train)
}

# Model pipeline
model = make_pipeline(
    StandardScaler(),
    CatBoostClassifier(
        iterations=100,
        learning_rate=0.1,
        depth=6,
        eval_metric='AUC',
        verbose=0,
        random_state=42,
        class_weights=class_weights
    )
)

# EÄŸitim
model.fit(X_train, y_train)

# Tahmin
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# DeÄŸerlendirme
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("ROC AUC:", roc_auc_score(y_test, y_pred_proba))


# In[71]:


import pandas as pd
import matplotlib.pyplot as plt

# Ã–zellik isimleri
feature_names = X_train.columns

# CatBoost model kÄ±smÄ±nÄ± al
catboost_model = final_model.named_steps['catboostclassifier']

# Feature importance deÄŸerleri
importances = catboost_model.get_feature_importance()

# DataFrame'e Ã§evir
feat_imp_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Ä°lk 10 Ã¶nemli Ã¶zelliÄŸi gÃ¶rselleÅŸtir
plt.figure(figsize=(10,6))
plt.barh(feat_imp_df['Feature'][:10][::-1], feat_imp_df['Importance'][:10][::-1])
plt.xlabel('Importance')
plt.title('Top 10 Feature Importances (CatBoost)')
plt.show()


# In[68]:


import joblib

# 1. Modeli final_model deÄŸiÅŸkenine ata (bunu sadece isimlendirme iÃ§in yapÄ±yoruz)
final_model = model

# 2. Modeli diskâ€™e kaydet
joblib.dump(final_model, 'catboost_churn_model.pkl')

print("Model baÅŸarÄ±yla kaydedildi.")


# ## Model Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±
# 
# FarklÄ± modellerin performanslarÄ± aÅŸaÄŸÄ±daki metriklerle karÅŸÄ±laÅŸtÄ±rÄ±lmÄ±ÅŸtÄ±r:
# 
# - Accuracy (DoÄŸruluk)  
# - F1-Score (Pozitif sÄ±nÄ±f iÃ§in)  
# - ROC AUC  
# - Recall (Pozitif sÄ±nÄ±f iÃ§in)  
# 
# AÅŸaÄŸÄ±daki tablo ve grafikler, modellerin genel performansÄ±nÄ± Ã¶zetlemektedir.
# 

# In[64]:


import pandas as pd

# Model karÅŸÄ±laÅŸtÄ±rma verileri
model_results = pd.DataFrame({
    'Model': [
        'Logistic Regression',
        'Random Forest',
        'XGBoost',
        'LightGBM',
        'CatBoost'
    ],
    'Accuracy': [0.73, 0.75, 0.74, 0.74, 0.74],
    'F1 Score': [0.61, 0.70, 0.70, 0.70, 0.70],
    'ROC AUC': [0.83, 0.83, 0.80, 0.82, 0.83],
    'Recall (Class 1)': [0.80, 0.70, 0.67, 0.70, 0.76],
    'Precision (Class 1)': [0.50, 0.52, 0.51, 0.51, 0.51]
})

# GÃ¶rÃ¼ntÃ¼le
model_results


# In[92]:


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Model performans sonuÃ§larÄ±
data = {
    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'LightGBM', 'CatBoost'],
    'Accuracy': [0.73, 0.75, 0.74, 0.74, 0.74],
    'F1 Score': [0.61, 0.60, 0.61, 0.59, 0.61],
    'ROC AUC': [0.83, 0.83, 0.80, 0.82, 0.83],
    'Recall': [0.79, 0.70, 0.67, 0.70, 0.76]
}

df_perf = pd.DataFrame(data)

# Tabloyu gÃ¶ster
print(df_perf)

# Grafik Ã§izimi iÃ§in DataFrame'i uzun formata Ã§evir
df_melt = df_perf.melt(id_vars='Model', var_name='Metric', value_name='Score')

plt.figure(figsize=(10,6))
sns.barplot(data=df_melt, x='Metric', y='Score', hue='Model')
plt.title('Model Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±')
plt.ylim(0,1)
plt.legend(loc='lower right')
plt.show()


# ## Modelde En Ã–nemli 5 Ã–zellik
# 
# Bu grafik, modelin churn tahmininde en fazla etkisi olan ilk 5 Ã¶zelliÄŸi gÃ¶stermektedir. Ã–zelliklerin Ã¶nem derecesi, model tarafÄ±ndan tahmin performansÄ±na katkÄ±larÄ± dikkate alÄ±narak sÄ±ralanmÄ±ÅŸtÄ±r.
# 
# - **tenure (mÃ¼ÅŸterinin hizmet sÃ¼resi)**: En yÃ¼ksek Ã¶neme sahip Ã¶zellik olarak Ã¶ne Ã§Ä±kmaktadÄ±r. Uzun sÃ¼reli mÃ¼ÅŸterilerin churn yapma olasÄ±lÄ±ÄŸÄ± genellikle daha dÃ¼ÅŸÃ¼ktÃ¼r.
# - **TotalCharges (toplam harcama)**: MÃ¼ÅŸterinin toplam yaptÄ±ÄŸÄ± harcama da churn riskini etkileyen Ã¶nemli bir faktÃ¶rdÃ¼r.
# - **MonthlyCharges (aylÄ±k harcama)**: MÃ¼ÅŸterinin aylÄ±k Ã¶deme tutarÄ± da churn riskini etkileyebilir.
# - **Contract_Two year (iki yÄ±llÄ±k sÃ¶zleÅŸme)**: Uzun vadeli sÃ¶zleÅŸme yapan mÃ¼ÅŸterilerin churn riski genellikle daha dÃ¼ÅŸÃ¼ktÃ¼r.
# - **InternetService_Fiber optic (fiber optik internet hizmeti)**: Bu hizmet tÃ¼rÃ¼nÃ¼ kullanan mÃ¼ÅŸterilerin churn davranÄ±ÅŸÄ± Ã¼zerinde belirleyici bir rol oynayabilir.
# 
# Bu Ã¶zelliklerin anlaÅŸÄ±lmasÄ±, mÃ¼ÅŸteri kaybÄ±nÄ± azaltmak iÃ§in hangi alanlarda iyileÅŸtirmeler yapÄ±labileceÄŸi konusunda deÄŸerli iÃ§gÃ¶rÃ¼ler sunar.

# In[94]:


# Ã–rnek feature importance dataframe
feature_importance = pd.DataFrame({
    'Feature': ['tenure', 'TotalCharges', 'MonthlyCharges', 'Contract_Two year', 'InternetService_Fiber optic'],
    'Importance': [0.17, 0.15, 0.10, 0.09, 0.06]
}).sort_values('Importance')

plt.figure(figsize=(8,5))
plt.barh(feature_importance['Feature'], feature_importance['Importance'])
plt.xlabel('Importance')
plt.title('Top 5 Ã–nemli Ã–zellik')
plt.show()


# ## Risk Segmentasyonu
# 
# Model tarafÄ±ndan tahmin edilen churn risk skorlarÄ± kullanÄ±larak mÃ¼ÅŸteriler Ã¼Ã§ segmente ayrÄ±lmÄ±ÅŸtÄ±r:
# 
# - Low Risk (DÃ¼ÅŸÃ¼k risk): risk skoru < 0.33  
# - Medium Risk (Orta risk): 0.33 â‰¤ risk skoru < 0.66  
# - High Risk (YÃ¼ksek risk): risk skoru â‰¥ 0.66  
# 
# Bu segmentler, mÃ¼ÅŸteri davranÄ±ÅŸlarÄ±na gÃ¶re hedefli stratejiler geliÅŸtirmek iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.

# In[76]:


# MÃ¼ÅŸteri baÅŸÄ±na churn olasÄ±lÄ±klarÄ±nÄ± hesapla (1. sÄ±nÄ±fÄ±n olasÄ±lÄ±ÄŸÄ±)
risk_scores = final_model.predict_proba(X_test)[:, 1]

# Orijinal X_test DataFrame'ine 'churn_risk' sÃ¼tununu ekle
X_test_with_risk = X_test.copy()
X_test_with_risk['churn_risk'] = risk_scores

# Ä°lk 5 satÄ±rÄ± gÃ¶ster
print(X_test_with_risk[['churn_risk']].head())


# In[80]:


# Risk segmentlerini oluÅŸtur
bins = [0, 0.33, 0.66, 1]
labels = ['Low Risk', 'Medium Risk', 'High Risk']

X_test_with_risk['risk_segment'] = pd.cut(X_test_with_risk['churn_risk'], bins=bins, labels=labels)

# Segment bazÄ±nda mÃ¼ÅŸteri sayÄ±sÄ± ve ortalama risk
segment_summary = X_test_with_risk.groupby('risk_segment', observed=True)['churn_risk'].agg(['count', 'mean']).reset_index()
segment_summary.columns = ['Risk Segment', 'Customer Count', 'Average Churn Risk']

print(segment_summary)


# In[91]:


# TÃ¼m sayÄ±sal sÃ¼tunlarÄ± seÃ§
numeric_cols = X_test_with_risk.select_dtypes(include='number').columns.tolist()

# Segment bazÄ±nda ortalama deÄŸerler
segment_profiles = X_test_with_risk.groupby('risk_segment', observed=True)[numeric_cols].mean()

print(segment_profiles)


# In[104]:


fig, ax1 = plt.subplots(figsize=(8,5))

sns.barplot(data=segment_summary, x='Risk Segment', y='Customer Count', ax=ax1, alpha=0.6)
ax1.set_ylabel('Customer Count', color='b')
ax1.set_title('Risk Segmentlerine GÃ¶re MÃ¼ÅŸteri DaÄŸÄ±lÄ±mÄ± ve Ortalama Churn Riski')

ax2 = ax1.twinx()
sns.lineplot(data=segment_summary, x='Risk Segment', y='Average Churn Risk', ax=ax2, sort=False, marker='o', color='r')
ax2.set_ylabel('Average Churn Risk', color='r')

plt.show()


# ## Risk Segmentlerine GÃ¶re DetaylÄ± Profil Analizi
# 
# Risk segmentlerine gÃ¶re mÃ¼ÅŸterilerin sayÄ±sal ve kategorik Ã¶zelliklerinin daÄŸÄ±lÄ±mÄ± incelenmiÅŸtir.
# 
# - SayÄ±sal deÄŸiÅŸkenlerde (Ã¶rneÄŸin tenure, MonthlyCharges) segment bazÄ±nda ortalamalar hesaplanmÄ±ÅŸtÄ±r.
# 
# Bu analizler, farklÄ± risk gruplarÄ±nÄ±n profilini anlamaya yardÄ±mcÄ± olur ve hedefli aksiyon planlamasÄ± iÃ§in temel oluÅŸturur.

# In[107]:


numeric_cols = X_test_with_risk.select_dtypes(include=['int64', 'float64']).columns.tolist()
numeric_cols = [col for col in numeric_cols if col != 'risk_segment']

num_summary = X_test_with_risk.groupby('risk_segment')[numeric_cols].mean()
print(num_summary)


# ## Aksiyon Ã–nerileri
# 
# ### High Risk Segment
# - Ã–zel indirim ve destek programlarÄ± uygulanmalÄ±.
# - Proaktif mÃ¼ÅŸteri iletiÅŸimi ve hÄ±zlÄ± sorun Ã§Ã¶zÃ¼mÃ¼ saÄŸlanmalÄ±.
# 
# ### Medium Risk Segment
# - Sadakat artÄ±rÄ±cÄ± kampanyalar dÃ¼zenlenmeli.
# - ÃœrÃ¼n/hizmet kullanÄ±mÄ± teÅŸvik edilmeli.
# 
# ### Low Risk Segment
# - Mevcut mÃ¼ÅŸteri memnuniyeti ve baÄŸlÄ±lÄ±ÄŸÄ± korunmalÄ±.
# - Yenilikler ve fÄ±rsatlarla mÃ¼ÅŸteri ilgisi canlÄ± tutulmalÄ±.

# ## SonuÃ§lar
# 
# - CatBoost modeli en yÃ¼ksek performansÄ± gÃ¶stermiÅŸtir.
# - Risk segmentasyonu, mÃ¼ÅŸteri kaybÄ±nÄ± Ã¶nleme stratejilerinde kullanÄ±labilir.
# - Aksiyon Ã¶nerileri ile mÃ¼ÅŸteri kaybÄ± azaltÄ±labilir ve mÃ¼ÅŸteri baÄŸlÄ±lÄ±ÄŸÄ± artÄ±rÄ±labilir.
# 
# ## Ä°leri Ã‡alÄ±ÅŸmalar
# 
# - Daha fazla Ã¶zellik mÃ¼hendisliÄŸi ile model iyileÅŸtirilebilir.
# - Modelin gerÃ§ek zamanlÄ± kullanÄ±mÄ± iÃ§in deploy planlanabilir.
# - MÃ¼ÅŸteri geri bildirimleriyle model gÃ¼ncellenebilir.

# In[ ]:




